\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}

\makeatletter
\ams@newcommand{\iiiiint}{\DOTSI\protect\MultiIntegral{5}}
\renewcommand{\MultiIntegral}[1]{%
  \edef\ints@c{\noexpand\intop
    \ifnum#1=\z@\noexpand\intdots@\else\noexpand\intkern@\fi
    \ifnum#1>\tw@\noexpand\intop\noexpand\intkern@\fi
    \ifnum#1>\thr@@\noexpand\intop\noexpand\intkern@\fi
    \ifnum#1>4 \noexpand\intop\noexpand\intkern@\fi % <---- added

    \noexpand\intop
    \noexpand\ilimits@
  }%
  \futurelet\@let@token\ints@a
}
\makeatother

\makeatletter
\ams@newcommand{\iiiiiint}{\DOTSI\protect\MultiIntegral{6}}
\renewcommand{\MultiIntegral}[1]{%
  \edef\ints@c{\noexpand\intop
    \ifnum#1=\z@\noexpand\intdots@\else\noexpand\intkern@\fi
    \ifnum#1>\tw@\noexpand\intop\noexpand\intkern@\fi
    \ifnum#1>\thr@@\noexpand\intop\noexpand\intkern@\fi
    \ifnum#1>4 \noexpand\intop\noexpand\intkern@\fi % <---- added
	\ifnum#1>5 \noexpand\intop\noexpand\intkern@\fi % <---- added
    \noexpand\intop
    \noexpand\ilimits@
  }%
  \futurelet\@let@token\ints@a
}
\makeatother

\usepackage{braket}

\newcommand{\RomanNumeralCaps}[1]
    {\MakeUppercase{\romannumeral #1}}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{float}
\usepackage{verbatim}
\usepackage{listings}
\usepackage{hyperref}

%bibliography packages, bibliography files are plain text files marked .bib 
%... in the same directory as the .tex file.
\usepackage[nottoc,numbib]{tocbibind}
\usepackage{cite}

\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}
\usepackage{amsmath}
\usepackage{graphicx}
\graphicspath{{C:/Users/adria/Pictures/FYS3150/}} 
\author{Adrian Martinsen Kleven, Simon Schrader}
\title{Project 3}

\lstset{
 	language =C++,   
    frame=tb, % draw a frame at the top and bottom of the code block
    tabsize=4, % tab space width
    showstringspaces=false, % don't mark spaces in strings
    numbers=left, % display line numbers on the left
    commentstyle=\color{green}, % comment color
    keywordstyle=\color{blue}, % keyword color
    stringstyle=\color{red} % string color
}

\begin{document}

\part*{-Project 3 - FYS3150/FYS4150-
}
{\large By Simon Schrader (4150), Adrian Kleven (3150) - autumn 2019
}
\tableofcontents

\listoffigures
\listoftables

 
\clearpage
 
\section{Abstract}
Determining the ground state correlation energy between two electrons in a helium atom can be done by evaluating a certain six- dimensional integral assuming that the electrons can be modelled separately, as two, single- particle wave functions of an electron in the hydrogen atom \cite{Problem_set_3}. This integral is also applicable in other aspects of quantum mechanics \cite{Problem_set_3}. For this reason, it's of great value to examine different approaches to solving such integrals, as well as those with higher degrees of freedom. To solve this integral, two different approaches relying on Gaussian quadrature and Monte- Carlo integration were used, as well as the implementation of parallelization to speed up the programs.
\section{Introduction} 
The purpose of this article is to apply different versions of Gaussian quadrature and Monte- Carlo integration in solving a six- dimensional integral to examine their times expenditure and accuracy, as well as the effect of parallelizing the C++ implementations of these methods.

\section{Methods}
The expectation value of the correlation energy between two electrons interacting under the Coulomb interacting is given by the integral
\begin{equation}\label{eq:correlationenergy}
   \langle \frac{1}{|{\bf r}_1-{\bf r}_2|} \rangle =
   \int_{-\infty}^{\infty} d{\bf r}_1d{\bf r}_2  e^{-2\alpha (r_1+r_2)}\frac{1}{|{\bf r}_1-{\bf r}_2|}.
\end{equation}
where 
$$
   {\bf r}_i =  x_i {\bf e}_x + y_i {\bf e}_y +z_i {\bf e}_z
$$
and
$$
r_i = \sqrt{x_i^2+y_i^2+z_i^2}.
$$
$\alpha = 2$, corresponding to the charge of the nucleus of the helium atom as given by \cite{Problem_set_3}.
\subsection{Gaussian quadrature}
The method of Gaussian quadrature allows one to approximate a function $f(x)$ with a polynomial $P_{2N-1}(x)$ of degree $2n-1$, using only $N$ mesh points. Said polynomial is constructed using sets of orthogonal polynomials, meaning they have the property that
\begin{equation}
\int_a^b P_i(x)P_j(x)dx = \delta_{ij}
\end{equation}
where $\big\{ P_k\hspace{1mm}|\hspace{1mm} k \in \mathbb N \big\}$ is the set of polynomials orthogonal in the domain $[a,b]$, $\delta_{ij}$ is the Kronecker- delta and $P_0(x)$ is normalized to be $1$. Since the polynomials $\big\{ P_k\hspace{1mm}|\hspace{1mm} k \in \{0,\cdots ,N\} \big\}$ constitute an orthogonal set, any polynomial of degree $N$ or less can be constructed by a linear combination them\\\\Suppose a function $f(x)$ is approximated by the polynomial $Q_{2N-1}$. Then
\begin{equation}\label{Eq:Poly approximation}
\int_{a}^bf(x)dx \approx \int_{a}^bQ_{2N-1}(x)dx=\int_{a}^b\left(P_N(x)Q_{N-1}(x)+R_{N-1}(x)\right)dx
\end{equation}
where, due to the orthogonality of the $P_k$'s, $Q_{2N-1}$ can be decomposed into $P_N$ $R_{N-1}$ and 
\begin{equation}\label{Eq:orthogonal polynomial expansion}
Q_{N-1} = \sum\limits_{k=0}^{N-1} \beta_k P_k.
\end{equation}
Then
$$
\sum\limits_{k=0}^{N-1} \left( \int_{a}^b \beta_k P_N(x) P_k(x)dx \right)+\int_{a}^bR_{N-1}(x)dx = \int_{a}^bR_{N-1}(x)dx
$$
so
\begin{equation*}
\int_{a}^bf(x)dx \approx \int_{a}^bR_{N-1}(x)dx = \sum\limits_{k=0}^{N-1}  \int_{a}^b \alpha_k P_k(x)dx
\end{equation*}
where $R_{N-1}$ has been expressed in terms of orthogonal polynomials as was done in equation \ref{Eq:orthogonal polynomial expansion}.
Inserting $P_0(x) = 1$ this expression can be rewritten, then simplified:
\begin{equation}\label{Eq: integral as a function of a0}
\int_{a}^bR_{N-1}(x)dx = \sum\limits_{k=0}^{N-1}  \int_{a}^b \alpha_kP_0(x) P_k(x)dx = \int_{a}^b \alpha_0 = \alpha_0(b-a).
\end{equation}
So it's only necessary to identify a single coefficient when evaluating this integral.\\Looking again at the polynomial $Q_{2N-1}(x)$ that serves as the approximation to the function $f(x)$. The points $x_n$ where $n \in \big\{0,\cdots,N-1\big\}$ are the zeros of $P_N$. Composing the polynomial as was done in equation \ref{Eq:Poly approximation}
\begin{equation*}
Q_{2N-1}(x) = P_N(x)Q_{N-1}(x)+R_{N-1}(x)
\end{equation*}
and assessing at the points $x_n$:
\begin{equation}\label{Eq:Q2n_1 and Rn_1}
Q_{2N-1}(x_n) = P_N(x_n)Q_{N-1}(x_n)+R_{N-1}(x_n) = R_{N-1}(x_n).
\end{equation}
So at the zeros $P_N$, the approximating polynomial $Q_{2N-1}(x)$ equals $R_{N-1}(x)$ which as the other polynomials, can be expressed as a linear combination of orthogonal polynomials as was done in equation \ref{Eq:orthogonal polynomial expansion}.
\begin{equation*}
R_{N-1}(x) = \sum\limits_{k=0}^{N-1} \alpha_k P_k(x).
\end{equation*} 
And at the points $x_n$:
\begin{equation}\label{Eq:Number 7}
R_{N-1}(x_n) = \sum\limits_{k=0}^{N-1} \alpha_k P_k(x_n)\hspace{6mm}n \in \big\{0,\cdots,N-1\big\}.
\end{equation}
As $\big\{ P_k\hspace{1mm}|\hspace{1mm} k \in \{0,\cdots,N\} \big\}$ are orthogonal polynomials,no one $P_k$ is linearly dependent on any others, thus $P_k(x_n)$ can be expressed as an invertible $N\times N$ matrix with matrix elements $P_{kn}$.\\\\Multiplying both sides of equation \ref{Eq:Number 7} by 
\begin{equation}
\sum\limits_{j=0}^{N-1}P_{jk}^{-1}
\end{equation}
yields
\begin{equation*}
\left( \sum\limits_{j=0}^{N-1}P_{jk}^{-1} \right)R_{N-1}(x_n) = \left( \sum\limits_{j=0}^{N-1}P_{jk}^{-1} \right)\sum\limits_{k=0}^{N-1} \alpha_k P_k(x_n)
\end{equation*}
which, due to the orthogonality of the column- vectors gives
\begin{equation}\label{Eq: for the coefficients a}
\sum\limits_{k=0}^{N-1}P_{nk}^{-1}R_{N-1}(x_k) = \alpha_n.
\end{equation}
Returning then to equation \ref{Eq: integral as a function of a0}, and inserting the expression \ref{Eq: for the coefficients a} for $n=0$.
\begin{equation}
\int_{a}^bR_{N-1}(x)dx = (b-a)\left( \sum\limits_{k=0}^{N-1}P_{0k}^{-1} R_{N-1}(x_k)\right).
\end{equation}
Remembering back to equation \ref{Eq:Q2n_1 and Rn_1} and labelling $\omega_k =(b-a)P_{0k}^{-1}$ as the weights, the final equation is arrived at:
\begin{equation}
\int_{a}^bf(x)dx \approx \int_{a}^bQ_{2N-1}(x)dx=\sum\limits_{k=0}^{N-1}\omega_k Q_{2N-1}(x_k)
\end{equation}
where $x_k$ are the $N$ zeros of the orthogonal polynomial $P_N$ of degree $N$, $Q_{2N-1}$ is the polynomial approximation of the original function $f(x)$ and $\omega_k$ are the associated weights.\\\\For the case of $-a=b=1$, the orthogonal polynomials in question are the Legendre polynomials, orthogonal in the interval $x\in \{-1,\cdots,1\}$. Other such polynomials, orthogonal on different intervals, such as the Laguerre polynomials can be applied in the same manner, although come attached with an associated weight function $\Big \{W(x)\hspace{1mm}|\hspace{1mm} W(x)>0,\hspace{1mm}x \in \{-1,\cdots,1\}\Big \}$, integrable on the interval $[a,b]$ such that the integral to be evaluated is 
$$
\int_a^b W(x)f(x)dx
$$\cite{Lecture_Notes_Fall_2015}.
\subsubsection{Gauss- Legendre quadrature}
Expressing the integral in equation \ref{eq:correlationenergy} in terms of $x_1,y_1,z_1$ and $x_2,y_2,z_2$:
\begin{equation}\label{big integral 1}
\iiiiiint\limits_{-\infty}^{\hspace{2.2mm}\infty} \frac{\exp \left( -4\left( \sqrt{x_1^2+y_1^2+z_1^2}+\sqrt{x_2^2+y_2^2+z_2^2} \right) \right)}{\sqrt{(x_1-x_2)^2+(y_1-y_2)^2+(z_1-z_2)^2}}dx_1 dx_2dy_1dy_2dz_1dz_2
\end{equation}
The Gauss- Legendre quadrature relies on the orthogonality of the Legendre polynomials in the domain $x \in \{-1,\cdots,1\}$. Therefore it's necessary to replace infinity with other bounds that still yield suitable results. Then, only a simple change of variables is required to bring the integration bounds to $[-1, 1]$:
$$
\lim\limits_{t \rightarrow \pm \infty} f(t) \approx f(\pm \lambda) 
$$
change of variable $ t = \lambda \tau$:
$$
\int\limits_{-\lambda}^\lambda f(t)dt = \lambda \int\limits_{-1}^1 f(\lambda \tau)d\tau
$$
Using this fact for integral \ref{big integral 1} gives
$$
\lambda^5\iiiiiint\limits_{-1}^{\hspace{2.2mm}1} \frac{\exp \left( -4\lambda\left( \sqrt{\chi_1^2+\upsilon_1^2+\zeta_1^2}+\sqrt{\chi_2^2+\upsilon_2^2+\zeta_2^2} \right) \right)}{\sqrt{(\chi_1-\chi_2)^2+(\upsilon_1-\upsilon_2)^2+(\zeta_1-\zeta_2)^2}}d\chi_1 d\chi_2d\upsilon_1d\upsilon_2d\zeta_1d\zeta_2
$$
and then renaming the integrand
\begin{equation}
g(\chi_1 ,\chi_2,\upsilon_1,\upsilon_2,\zeta_1,\zeta_2) = \frac{\exp \left( -4\lambda\left( \sqrt{\chi_1^2+\upsilon_1^2+\zeta_1^2}+\sqrt{\chi_2^2+\upsilon_2^2+\zeta_2^2} \right) \right)}{\sqrt{(\chi_1-\chi_2)^2+(\upsilon_1-\upsilon_2)^2+(\zeta_1-\zeta_2)^2}}
\end{equation}
gives the final equation
\begin{equation}
\lambda^5\iiiiiint\limits_{-1}^{\hspace{2.2mm}1}g(\chi_1 ,\chi_2,\upsilon_1,\upsilon_2,\zeta_1,\zeta_2)d\chi_1 d\chi_2d\upsilon_1d\upsilon_2d\zeta_1d\zeta_2.
\end{equation}
As long as the function $g$ can be approximated with a polynomial $P_{2N-1}$ of degree $2N-1$ or lower on the interval $[-1,1]$, this integral can be approximated by
\begin{equation}
\lambda^5 \sum\limits_{i_1=1}^{N}\cdots\sum\limits_{i_6=1}^{N}\omega_{i_1}\cdots \omega_{i_6} g(x_{i_1},\cdots,x_{i_1})
\end{equation}
where $\omega_{ik}$ for $k \in\{1,\cdots,6\}$ are the weights and $x_{ik}$ for $k \in\{1,\cdots,6\}$ are the zeros of the Legendre polynomial $\mathcal{L}_N$ of degree $N$.\\\\$\mathcal{L}_N(x_{ik})$ can be gotten by simply using the recursion relation
\begin{equation}
 (n+1)\mathcal{L}_{n+1}(x_{ik})+n\mathcal{L}_{n-1}(x_{ik})-(2n+1)x_{ik}\mathcal{L}_n(x_{ik})=0
\end{equation}
given by \cite{Lecture_Notes_Fall_2015}
and the requirement that $\mathcal{L}_0=1$.\\The zeros of $\mathcal{L}_N$, $x_{ik}$ are found by any root- finding algorithm, such as Newton's method. 
\subsubsection{Gauss- Laguerre quadrature}
The Gauss- Legendre quadrature in this case, got a little ugly. A different approach would be to transform the integral with polar coordinates and using the Laguerre polynomials that are orthogonal on the interval $[0,\infty]$.\\Looking again at equation \ref{eq:correlationenergy}, and applying the change of variables given in the Project 3 Problem set \cite{Problem_set_3} section 3b):
\begin{equation*}
\int_{0}^{\pi}\int_{0}^{\pi}\int_{0}^{2\pi}\int_{0}^{2\pi}\int_{0}^{\infty}\int_{0}^{\infty}
\frac{r_1^2r_2^2sin(\theta_1)sin(\theta_2)e^{-4(r_1+r_2)}}{\sqrt{r_1^2+r_2^2-2r_1r_2cos(\beta)}}dr_1dr_2d\phi_1d\phi_2d\theta_1d\theta_2
\end{equation*}
with
\begin{equation*}
cos(\beta)=cos(\theta_1)cos(\theta_2)+sin(\theta_1)sin(\theta_2)cos(\phi_1-\phi_2).
\end{equation*}
$r_1$ and $r_2$ in this case, are integrated from $0$ to $\infty$, suitable for the Gauss- Laguerre quadrature.\\The Gauss- Laguerre quadrature carries the weight function
\begin{equation}
W(x)=x^\alpha e^{-x}
\end{equation}
\cite{Lecture_Notes_Fall_2015}. $\alpha$ being the exponent of $x$ in an integral of the form
\begin{equation}
\int_0^{\infty}x^{\alpha}e^{-x}f(x).
\end{equation}
For simplicity, lets isolate one of the $r$'s in the integrand, as the other will be treated as a constant. let's pick $r_1$:
\begin{equation}\label{laguerre integral with just r1}
C\int_{0}^{\infty}\frac{r_1^2e^{-4r_1}}{\sqrt{r_1^2+r_2^2-2r_1r_2cos(\beta)}}dr_1
\end{equation}
where
\begin{equation*}
C = \int_{0}^{\pi}\int_{0}^{\pi}\int_{0}^{2\pi}\int_{0}^{2\pi}\int_{0}^{\infty} r_2^2sin(\theta_1)sin(\theta_2)e^{-4r_2)}dr_2d\phi_1d\phi_2d\theta_1d\theta_2.
\end{equation*}
The integral \ref{laguerre integral with just r1}, can then be written as
\begin{equation}\label{Laguerre integral (r1) expresses as weight and function }
C\int_{0}^{\infty}W(r_1)g(r_1)dr_1
\end{equation}
with
\begin{equation*}
W(r_1)=r_1^2 e^{-r_1}
\end{equation*}
and
\begin{equation*}
g(r_1)=\frac{e^{-3r_1}}{\sqrt{r_1^2+r_2^2-2r_1r_2cos(\beta)}}.
\end{equation*}
Integral \ref{Laguerre integral (r1) expresses as weight and function }, neglecting $C$ for just a bit, can now be approximated according to the Gauss- Laguerre quadrature:
\begin{equation}
\int_{0}^{\infty}W(r_1)g(r_1)dr_1 \approx \sum\limits_{i=1}^N \omega_i g(x_i)
\end{equation}
$\omega_i$ being the weights acquired by inverting the matrix (see appendix \ref{Proof that big ass matrix is invertible})
\begin{equation}\label{Big ass matrix}
\textbf{L}=
\begin{bmatrix}
e^{-x_{1}}L_{0}(x_{1}) & e^{-x_{1}} L_{1}( x_{1}) & \cdots &\cdots & e^{-x_{1}} L_{N-1} (x_{1})\\\ e^{-x_{2}} L_{0}( x_{2}) & e^{-x_{2}} L_{1}( x_{2}) & \cdots & \cdots &e^{-x_{2}} L_{N-1}( x_{2})\\\ \vdots & \vdots & \vdots & \vdots & \vdots & \\\ e^{-x_{N-1}} L_{0}( x_{N-1}) & e^{-x_{N-1}} L_{1}( x_{N-1}) & \cdots & \cdots & e^{-x_{N-1}} L_{N-1}( x_{N-1})\\\ e^{-x_{N}} L_{0}( x_{N}) & e^{-x_{N}} L_{1}( x_{N}) & \cdots & \cdots & e^{-x_{N}} L_{N-1}( x_{N})
\end{bmatrix}
\end{equation}
and $x_i | i \in \{1,\cdots,N \}$ being the zeros of the Laguerre polynomial $L_N$ of degree $N$.
\subsection{Monte Carlo Integration}
A different approach to solving integrals numerically is the use of Monte Carlo integration, where properties of probability distribution functions (PDFs) are used to approximate the solution. For any integral $\int_a^bf(x)dx$, one can find a PDF p(x) that fulfills  $\int_a^bp(x)dx=1$ that is nonzero $ \forall x\in [a,b]$ Then by the law of large numbers \cite{devore2012modern},
$$\int_a^bf(x)dx=\int_a^bp(x)\frac{f(x)}{p(x)}dx=E[\frac{f(x)}{p(x)}]\approx \frac{1}{N}\sum_{i=1}^{N}\frac{f(x_i)}{p(x_i)}$$
where N is a very large number and $x_i$ are random samples from the given PDF.\\
The variance is then given by
\[ 
\sigma^2=\frac{1}{N}\sum_{i=1}^{N}\left(\left( \frac{f(x_i)}{p(x_i)}\right)^2-E[\frac{f(x)}{p(x)}]^2\right) p(x_i)=E[ ( \frac{f(x)}{p(x)})^2]-E[\frac{f(x)}{p(x)}]^2
\]
It can then be shown \cite{devore2012modern} that the standard error of the mean is
$$
\sigma_N \approx \frac{\sigma}{\sqrt{N}}
$$
Thus, the error is a function of $\frac{1}{\sqrt{N}}$. This is does not depend on the dimensionality of the integral to be evaluated, making Monte Carlo methods very effective for integrals in higher dimensions.
The appropriate choice of a fitting PDF is crucial. Even though the above equations hold true for any PDF, choosing a PDF p(x) that closely follows our function f(x), leads to a better sampling of $x_i$ \cite{devore2012modern} and decreases the standard deviation. Thus, the real integral is approached much faster.

\subsection{Implementation}

\subsubsection{Legendre polynomials}
The first approach to solving the integral is to use Legendre polynomials. As Legendre polynomials cannot be properly mapped to $(-\infty,\infty)$, it is necessary to define a threshold $\lambda$ where the function is "sufficiently" zero. Thus, the integral is only evaluated for $(-\lambda,\lambda)$ As can be seen in FIGURE XXX, the function approaches zero rather quickly, and $\lambda$=2 seems to be an appropriate choice. The integration limit is then changed to $[-2,2]$ for all 6 integrands. Numerical recipe's gauleg-function \cite{press1992numerical} then maps the weights from $[-1,1]$ to $[-2,2]$. Because any all the 6 variables in the integral are freely interchangeable, the the approximation then reads, 
$$\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x_1,x_2,y_1,y_2,z_1,z_2)dx_1dx_2dy_1dy_2dz_1dz_2$$
$$\approx\int_{-2}^{2}\int_{-2}^{2}\int_{-2}^{2}\int_{-2}^{2}\int_{-2}^{2}\int_{-2}^{2}f(x_1,x_2,y_1,y_2,z_1,z_2)dx_1dx_2dy_1dy_2dz_1dz_2 $$
$$\approx\sum_{i=1}^{N}\sum_{j=1}^{N}\sum_{k=1}^{N}\sum_{l=1}^{N}\sum_{m=1}^{N}\sum_{n=1}^{N}\omega_i\omega_j\omega_k\omega_l\omega_m\omega_nf(x_i,x_j,x_k,x_l,x_m,x_n)$$
where $x_i$ and $\omega_i$ where created using the gauleg-function. 
Computationally, this will lead to a total of $N^6$ function evaluations. A possible problem to face is that one will end up dividing by zero $N^3$ times. This problem can be faced by ignoring all function evaluations where the function's denominator is lower than a threshold, which was chosen to be $10^{-8}.$
\subsubsection{Laguerre and Legendre polynomials}
When transferred into spherical coordinates, the integral reads 

$$
\int_{0}^{\pi}\int_{0}^{\pi}\int_{0}^{2\pi}\int_{0}^{2\pi}\int_{0}^{\infty}\int_{0}^{\infty}
\frac{r_1^2r_2^2sin(\theta_1)sin(\theta_2)e^{-4(r_1+r_2)}}{\sqrt{r_1^2+r_2^2-2r_1r_2cos(\beta)}}dr_1dr_2d\phi_1d\phi_2d\theta_1d\theta_2
$$
with
$$
cos(\beta)=cos(\theta_1)cos(\theta_2)+sin(\theta_1)sin(\theta_2)cos(\phi_1-\phi_2)
$$

This has the advantage that infinity only has to be faced twice, and Laguerre-Polynomials can be used for that. Laguerre-polynomials are defined for $[0,\infty)$ and have a weight function $W(x)=e^{-x}$. This is relevant for $r_1$ and $r_2$, as the integration limits go from $0$ to $\infty$. The angles $\theta_1$ and $\theta_2$ lie in $[0,\pi]$, and the angles $\phi_1$ and $\phi_2$  lie in $[0,2\pi]$. Here, Legendre-Polynomials can be used again. Because each of the $\theta$, $\phi$ and $r$ are interchangeable, it is necessary to create 3 types of mesh points and weights: One using the Laguerre polynomials  ($\omega_r$, $x_r$), one using Legendre polynomials for the angle $\theta$ ($\omega_\theta$, $x_\theta$) and one using Legendre polynomials for the angle $\phi$ ($\omega_\phi$, $x_\phi$). The function to be evaluated changes slightly due to the Laguerre-polynomials weight function, the exponent goes from -4 to -3, leading to
$$g(r_1,r_2,\theta_1,\theta_2,\phi_1,\phi_2)=\frac{r_1^2r_2^2sin(\theta_1)sin(\theta_2)e^{-3(r_1+r_2)}}{\sqrt{r_1^2+r_2^2-2r_1r_2cos(\beta)}}$$
$$\int_{0}^{\pi}\int_{0}^{\pi}\int_{0}^{2\pi}\int_{0}^{2\pi}\int_{0}^{\infty}\int_{0}^{\infty}
g(r_1,r_2,\theta_1,\theta_2,\phi_1,\phi_2)dr_1dr_2d\phi_1d\phi_2d\theta_1d\theta_2$$
This integral is then approximated by the following sum:

$$
\approx \sum_{i=1}^N \sum_{j=1}^N \sum_{k=1}^N \sum_{l=1}^N \sum_{m=1}^N \sum_{n=1}^N \omega_{r,i} \omega_{r,j} \omega_{\theta ,k} \omega_{\theta ,l} \omega_{\phi ,m} \omega_{\phi ,n} g(x_{r,i},x_{r,j},x_{\theta ,k},x_{\theta ,l},x_{\phi ,m},x_{\phi ,n})
$$

\section{Results}

\section{Conclusion}

\section{Critique}

\section{Appendix}
\subsection{Proof that \textbf{L} is invertible}\label{Proof that big ass matrix is invertible}
For the matrix $\textbf{L}$ in equation \ref{Big ass matrix}; each column is constructed of constants multiplied with the Laguerre polynomial $L_n$ for each row in column $n$. With the Laguerre polynomials being orthogonal with respect to the inner product
\begin{equation}
\braket{L_n|L_m} = \int\limits_0^\infty e^{-x} L_n(x) L_m(x) dx = \delta_{mn}
\end{equation}
no column can be expressed as a linear combination of any of the others (at least not for $x \in [0,\infty]$, so \textbf{L} is an $N\times N$ matrix with $N$ linearly independent columns. As such it is necessarily non-singular and therefore, must be invertible.
\subsection{List of programs}
All programs can be found on \url{https://github.com/adrian2208/FYS3150_collab} in the folder "project 3".
\begin{itemize}
\item[1.] 
\end{itemize}

\subsection{Tables}

\cite{Lecture_Notes_Fall_2015}
\cite{Problem_set_3}
\bibliographystyle{Plain}



\bibliography{citations}



\begin{comment}

$$
\begin{bmatrix}
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
\end{bmatrix}
$$

\begin{lstlisting}[caption=insert caption]
for (unsigned int i = 0; i<100;i++{
}
\end{lstlisting}

\begin{figure}[h]
\includegraphics[width=8cm]{}
\caption{include caption}
\end{figure}

\end{comment}

\end{document}